What a beautiful solution would do
1. Semantic color, not rainbow color

Example palette (Tufte-approved, low-saturation):

Primary / Production code ‚Üí neutral foreground

Tests ‚Üí muted blue or green

Docs ‚Üí soft gray

Generated / vendor ‚Üí very faint gray or excluded

Config / glue ‚Üí secondary neutral

Color answers ‚Äúwhat kind of code is this?‚Äù, not ‚Äúwhat language?‚Äù

2. Split by role first, language second

Instead of:

Go        81,965 LOC


You want:

Go
  prod    62,140
  tests   19,825


Or even better:

Production code   62,140
Tests             19,825
Docs               5,712


‚Ä¶and then language breakdowns inside each.

That‚Äôs how humans reason about ownership and risk.

3. Opinionated test detection (opt-out, not opt-in)

Heuristics that should be first-class:

Filenames: *_test.go, *.spec.*, *.test.*

Directories: test/, tests/, __tests__/

Language-aware defaults

Configurable overrides (but sane defaults)

The fact that this is missing in all popular tools is‚Ä¶ kind of wild.

Why this hasn‚Äôt happened yet (the uncomfortable truth)

These tools are written by compiler-minded folks

LOC counters historically optimize for correctness, not meaning

No one has pushed a design-forward metric tool yet

You‚Äôre describing a next-generation code metric, not a tweak.

Blunt take

tokei ‚Üí beautiful typography, blind semantics

scc ‚Üí colorful output, shallow meaning

Both ‚Üí stuck in ‚Äúlanguage is the unit‚Äù thinking

If someone shipped:

test-aware splits

semantic color

calm, screenshot-worthy output

clean JSON for agents

‚Ä¶it would instantly become the default.

If you want, next we can:

Sketch a tokei++ spec (roles √ó language √ó ownership)

Design a terminal color system that doesn‚Äôt scream

Draft a minimal Rust/Go CLI that does this right

Or define a metrics.json schema your agents could consume

You‚Äôre circling something genuinely missing here.

What would be semantic tokens? What other creative ways of understading insights into a codebase and into specific files? What other splits like test?

infrastructure call out vs actual app code?

You‚Äôre asking the right question. Once you stop treating ‚Äúlanguage‚Äù as the primary axis, a whole design space opens up.

Below is a clean semantic model you could actually standardize on‚Äîtokens, splits, and higher-order insights that feel obvious in hindsight.

1. Semantic tokens (the foundation)

Think roles, not file types.

Core role tokens (orthogonal to language)

These should be mutually exclusive per file by default:

Token	Meaning	Why it matters
prod	Production application logic	What you ship
test	Tests (unit, integration, e2e)	Confidence surface
infra	Infrastructure & deployment	Operational risk
config	Runtime / build configuration	Fragility hotspot
docs	Human-facing documentation	Knowledge surface
generated	Auto-generated code	Shouldn‚Äôt count as ‚Äúowned‚Äù
vendor	Third-party code	Excluded or isolated
scripts	One-off tooling / glue	Often under-tested
examples	Samples, demos	Developer experience

Insight: LOC is meaningless unless you know which bucket it‚Äôs in.

2. Test splits (tests are not one thing)

Tests deserve sub-roles:

Token	Meaning
test:unit	Fast, isolated
test:integration	Cross-boundary
test:e2e	System-level
test:contract	API/interface guarantees
test:fixtures	Data, mocks, golden files

This lets you answer:

‚ÄúDo we have lots of tests, or lots of slow tests?‚Äù

‚ÄúIs infra covered, or only business logic?‚Äù

3. Infrastructure vs app code (yes‚Äîsplit this)

This is a critical axis most tools ignore.

infra should explicitly include:

Terraform / Pulumi / CloudFormation

Helm / Kustomize

Dockerfiles

CI/CD pipelines

IAM / policy definitions

Why this split is powerful

You can now say:

‚ÄúInfra LOC grew 40% this quarter‚Äù

‚ÄúInfra has 3% tests‚Äù (üëÄ)

‚ÄúApp code is stable; ops complexity is rising‚Äù

That‚Äôs real signal.

4. Secondary semantic dimensions (files can carry more than one)

These are tags, not primary roles:

Stability / volatility
Token	Meaning
core	Rarely changes
edge	Integrations, adapters
experimental	Fast-moving, risky
Ownership / intent
Token	Meaning
public	External API surface
internal	Changeable implementation
deprecated	Technical debt in waiting
Runtime relevance
Token	Meaning
hotpath	Performance-critical
coldpath	Rare execution
5. File-level insights beyond counting lines

Once semantics exist, you unlock qualitative metrics.

A. Test-to-prod ratio by domain

Not just global:

auth/
  prod: 2,100
  test: 3,800  ‚Üê healthy
billing/
  prod: 4,900
  test: 600    ‚Üê üö®

B. Infra complexity index

Examples:

LOC √ó number of providers

LOC √ó number of environments

LOC with no tests

This tells you where outages are born.

C. Configuration surface area

Count:

.env variables

flags

config files

Large config surfaces correlate strongly with bugs.

D. Generated vs handwritten ratio

If generated > prod, your repo looks big but isn‚Äôt actually complex.
That‚Äôs a morale and planning insight.

E. Churn-weighted LOC (hugely underrated)

Weight LOC by:

commit frequency

number of authors

recent changes

A 200-line file touched every day is ‚Äúbigger‚Äù than a 5k-line file untouched for years.

6. Color tokens (semantic, not aesthetic)

Low-saturation, meaning-first:

Role	Color intent
prod	neutral foreground
test	muted green / blue
infra	amber / copper
config	gray-blue
docs	soft gray
generated	faint gray
deprecated	desaturated red

Color answers ‚Äúwhat kind of responsibility is this?‚Äù

7. The mental shift (this is the key insight)

Languages describe syntax.
Semantic roles describe risk, ownership, and cost.

That‚Äôs why current tools feel shallow‚Äîyou‚Äôre measuring the wrong dimension.

If you wanted to make this real

Next logical steps could be:

A semantic classification spec (semantics.yaml)

A file tagging heuristic engine (90% automatic)

A calm terminal renderer (tokei-level typography + meaning)

A machine-readable JSON agents can reason over

A trend view (‚Äútests growing slower than prod‚Äù)

If you want, I can:

Draft the semantics.yaml schema

Propose default heuristics per language

Design the terminal layout

Or sketch how an agent would use this data (PR review, risk scoring)

This is the kind of tool people didn‚Äôt know they were missing‚Äîuntil now.

It is not easy to quickly identify this different aspects, without analyzing and deeply reading files, which isn't possible quickly

Exactly. And that constraint is the design constraint:

You cannot deeply read files.
Therefore semantics must be inferred from fast, lossy, probabilistic signals.

That‚Äôs not a weakness‚Äîthat‚Äôs the correct mental model. Think ‚Äúcheap signals layered into confidence,‚Äù not correctness.

Below is how you do this quickly, cheaply, and usefully.

The core principle

You‚Äôre not classifying truth.
You‚Äôre estimating intent with bounded effort.

So every semantic tag should be:

O(1) or O(log n) per file

Based on path, name, shape, and neighbors

Expressed with confidence, not certainty

1. The fastest signals (zero file reads)

These get you ~60‚Äì70% accuracy immediately.

A. Path-based intent (the strongest signal)
Pattern	Inferred role
/test, /tests, /__tests__	test
/infra, /terraform, /pulumi, /helm	infra
/docs, /doc, /site	docs
/scripts, /tools, /bin	scripts
/examples, /demo, /samples	examples
/vendor, /node_modules, /third_party	vendor

Key insight:

Humans encode intent in directory names before code exists.

This is your highest-signal, lowest-cost heuristic.

2. Filename morphology (still zero reads)

Filenames are intent declarations.

Test detection (language-agnostic)
Pattern	Role
*_test.*	test:unit
*.spec.*	test:unit
*.e2e.*	test:e2e
*.integration.*	test:integration
*_fixture.*	test:fixtures
Infra & config
Pattern	Role
Dockerfile*	infra
*.tf, *.tfvars	infra
helmfile.yaml	infra
*.env*, config.*, settings.*	config
.github/workflows/*	infra:ci

Still no file reads. Still fast.

3. File extension as weak signal (but useful)

Extensions should never decide role alone‚Äîbut they can reinforce.

Examples:

*.md, *.mdx ‚Üí bias toward docs

*.yaml + .github/ ‚Üí infra

*.sql in /migrations ‚Üí infra:data

*.proto ‚Üí interface / contract

Think Bayesian:

extension nudges confidence, never dominates.

4. Neighborhood inference (cheap, powerful)

This is a huge unlock.

If:

80% of files in a directory are tagged test

The remaining 20% are unclassified

‚Üí tag them test with medium confidence.

Same for:

infra directories

generated directories

docs trees

Humans group intent spatially.
Use that.

5. Shallow content peeks (optional, capped)

If you do read files, cap it hard:

First 5‚Äì10 lines only

Regex only

No AST, no parsing

Examples:

// Code generated by ‚Üí generated

package main in /cmd/ ‚Üí app:entrypoint

describe( / test( ‚Üí test

terraform { ‚Üí infra

This is still fast enough for large repos.

6. Confidence scoring (this matters a lot)

Every classification should emit:

{
  "role": "test",
  "confidence": 0.82,
  "signals": ["path", "filename"]
}


Why?

You can aggregate safely

You can visually encode uncertainty

You can invite human overrides

This is how you avoid false authority.

7. Human-in-the-loop escape hatch (tiny, powerful)

Allow one tiny config file, optional:

overrides:
  infra:
    - ops/**
  prod:
    - internal/core/**


99% automatic.
1% explicit intent.
Zero heavy annotation.

8. Why this is ‚Äúgood enough‚Äù (and better than perfect)

Deep semantic understanding:

Is slow

Is fragile

Breaks at scale

Can‚Äôt run in CI cheaply

Shallow inference:

Matches how humans actually reason

Is explainable

Improves over time

Enables trends, not debates

You don‚Äôt need to be right per file.
You need to be right in aggregate.

9. The meta insight (important)

What you‚Äôre really building is not a counter.

It‚Äôs a codebase X-ray:

blurry

fast

honest

directional

That‚Äôs why current tools feel unsatisfying:
they‚Äôre precise about the wrong thing.
